%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}




\title{ENIIGMA code}
\date{May 13, 2020}
\release{0.0}
\author{ENIIGMA Team}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Introduction}
\label{\detokenize{index:introduction}}
ENIIGMA is a python code focused on the dEcomposition of Infrared Ice Spectra using Genetic Modelling Algorithm.

An infrared ice spectra library is included in the code directory and used to decompose either observational or experimental ice spectra.

The genetic algorithm module run the Pyevolve Python package and give the global minimum solution for the spectral decomposition after several generations.

Once the ENIIGMA code has finished the spectral decomposition, an statistical is called to perform 1. Confidence analysis interval, 2. Ice column density calculation,
and 3. Degeneracy analysys by Pie chart and histograms.

Other functionalities of the ENIIGMA code regards to the spectral continuum calculation by polynomial and blackbody fitting.


\chapter{Features:}
\label{\detokenize{index:features}}
The ENIIGMA code is focused on the spectral decomposition of the observational IR spectra containing absorption ice features. So far, the \sphinxstyleemphasis{Spitzer} and VLT/ISAAC spectra
have been successfully tested. In this regard, this code will be useful to provide an unbiased analysis of the upcoming \sphinxstyleemphasis{James Webb Space Telescope \sphinxhyphen{} JWST} data. Nevertheless,
laboratory ice spectra can be decomposed using the ENIIGMA code. The current and planned technical features are listed below:


\section{Before you start}
\label{\detokenize{beforeyoubegin:before-you-start}}\label{\detokenize{beforeyoubegin:settingup}}\label{\detokenize{beforeyoubegin::doc}}

\subsection{Required Python packages:}
\label{\detokenize{beforeyoubegin:required-python-packages}}\begin{itemize}
\item {} 
Python (\sphinxurl{http://www.python.org/}): tested with 2.7.10 and 3.7.7

\item {} 
Numpy (\sphinxurl{http://www.numpy.org}): tested with 1.16+

\item {} 
Pandas (\sphinxurl{https://pandas.pydata.org}): tested with 0.25+

\item {} 
LMFIT (\sphinxurl{https://lmfit.github.io/lmfit-py/index.html}): tested with 0.9+

\item {} 
Pyevolve \sphinxhyphen{} Python 2.7 (\sphinxurl{http://pyevolve.sourceforge.net/0\_6rc1/index.html}): tested with 0.6+

\item {} 
Pyevolve \sphinxhyphen{} Python 3+ (\sphinxurl{https://github.com/BubaVV/Pyevolve}): tested with 0.6+

\item {} 
Matplotlib (\sphinxurl{https://matplotlib.org}): tested with 2.2.3 and 3.0.2

\item {} 
Scipy (\sphinxurl{https://www.scipy.org}): tested with 1.1+

\item {} 
sh (\sphinxurl{https://pypi.org/project/sh/}): tested with 1.12.14

\end{itemize}


\subsection{Installation:}
\label{\detokenize{beforeyoubegin:installation}}
The installation can be done by typing the following command in shell:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{}pip install \PYGZhy{}i https://test.pypi.org/simple/ eniigma\PYGZhy{}try3
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
Obs: Pyevolve in Python 3+ must be downloaded and installed via the following commands:
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{}git clone https://github.com/BubaVV/Pyevolve.git
\PYGZdl{}cd Pyevolve
\PYGZdl{}pip3 install future
\PYGZdl{}sudo python setup.py install \PYGZhy{}\PYGZhy{}
\end{sphinxVerbatim}


\section{Importing modules}
\label{\detokenize{Import_modules:importing-modules}}\label{\detokenize{Import_modules::doc}}

\subsection{Continuum}
\label{\detokenize{Import_modules:continuum}}
Both polynomial and blackbody continuum fit are imported via:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{Continuum} \PYG{k+kn}{import} \PYG{n}{Fit}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Fit}\PYG{o}{.}\PYG{n}{Continuum\PYGZus{}poly}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+m+mf}{5.15}\PYG{p}{,} \PYG{l+m+mf}{30.}\PYG{p}{,} \PYG{n}{order} \PYG{o}{=} \PYG{l+m+mf}{1.}\PYG{p}{,} \PYG{n}{range\PYGZus{}limits}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{5.25}\PYG{p}{,} \PYG{l+m+mf}{5.30}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{26.5}\PYG{p}{,} \PYG{l+m+mf}{27.5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{27.5}\PYG{p}{,}\PYG{l+m+mf}{30.}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Fit}\PYG{o}{.}\PYG{n}{Continuum\PYGZus{}BB}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+m+mf}{5.15}\PYG{p}{,} \PYG{l+m+mf}{30.}\PYG{p}{,} \PYG{n}{range\PYGZus{}limits}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{5.25}\PYG{p}{,} \PYG{l+m+mf}{8.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{16.}\PYG{p}{,} \PYG{l+m+mf}{20.}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{20.}\PYG{p}{,} \PYG{l+m+mf}{30.}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{guess} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{l+m+mf}{1e\PYGZhy{}18}\PYG{p}{,} \PYG{l+m+mi}{200}\PYG{p}{,} \PYG{l+m+mf}{3e\PYGZhy{}16}\PYG{p}{)}\PYG{p}{,} \PYG{n}{guess\PYGZus{}view} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Silicate feature decomposition}
\label{\detokenize{Import_modules:silicate-feature-decomposition}}
The silicate features at 9.8 and 18 microns are decomposed by 6 Gaussian function each one:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{Silicate} \PYG{k+kn}{import} \PYG{n}{Silicate\PYGZus{}deconv}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Silicate\PYGZus{}deconv}\PYG{o}{.}\PYG{n}{Silicate\PYGZus{}decomposition}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{p}{,} \PYG{n}{npoints} \PYG{o}{=} \PYG{l+m+mf}{1000.}\PYG{p}{,} \PYG{n}{pathlib} \PYG{o}{=} \PYG{n}{Default}\PYG{p}{,} \PYG{n}{silicate\PYGZus{}guess\PYGZus{}factor} \PYG{o}{=} \PYG{l+m+mf}{0.25}\PYG{p}{,} \PYG{n}{values\PYGZus{}wid} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{l+m+mf}{0.9}\PYG{p}{]}\PYG{p}{,} \PYG{n}{perc\PYGZus{}wid\PYGZus{}silicate}\PYG{o}{=}\PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{n}{perc\PYGZus{}wid\PYGZus{}data} \PYG{o}{=} \PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{n}{perc\PYGZus{}amp\PYGZus{}min\PYGZus{}data}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{n}{perc\PYGZus{}amp\PYGZus{}max\PYGZus{}data}\PYG{o}{=}\PYG{l+m+mf}{0.41}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Genetic algorithm optimisation}
\label{\detokenize{Import_modules:genetic-algorithm-optimisation}}
Called as:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{GA} \PYG{k+kn}{import} \PYG{n}{optimize}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{optimize}\PYG{o}{.}\PYG{n}{ENIIGMA}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+m+mf}{2.5}\PYG{p}{,}\PYG{l+m+mf}{15.}\PYG{p}{,} \PYG{n}{list\PYGZus{}sp}\PYG{p}{,} \PYG{n}{group\PYGZus{}comb}\PYG{o}{=}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{skip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{pathlib} \PYG{o}{=} \PYG{n}{Default}\PYG{p}{)}
\end{sphinxVerbatim}

Once finished the optimisation, the fitness function evolution can be checked as:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{GA} \PYG{k+kn}{import} \PYG{n}{check\PYGZus{}ga}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{check\PYGZus{}ga}\PYG{o}{.}\PYG{n}{check}\PYG{p}{(}\PYG{n}{combination}\PYG{o}{=}\PYG{l+m+mi}{185}\PYG{p}{,} \PYG{n}{option}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{9}\PYG{p}{)}
\end{sphinxVerbatim}

The best five combinations can be checked by:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{GA} \PYG{k+kn}{import} \PYG{n}{check\PYGZus{}ga}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{check\PYGZus{}ga}\PYG{o}{.}\PYG{n}{top\PYGZus{}five\PYGZus{}scaled}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Statistical analysis}
\label{\detokenize{Import_modules:statistical-analysis}}
The statistical module is imported from ENIIGMA.Stats. The following options are available:

Pie chart

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{Stats} \PYG{k+kn}{import} \PYG{n}{Pie\PYGZus{}chart\PYGZus{}plots}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Pie\PYGZus{}chart\PYGZus{}plots}\PYG{o}{.}\PYG{n}{pie}\PYG{p}{(}\PYG{n}{sig\PYGZus{}level}\PYG{o}{=}\PYG{l+m+mf}{16.81}\PYG{p}{)}
\end{sphinxVerbatim}

Confidence interval

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{Stats} \PYG{k+kn}{import} \PYG{n}{Stats\PYGZus{}Module}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Stats\PYGZus{}Module}\PYG{o}{.}\PYG{n}{stat}\PYG{p}{(}\PYG{n}{f\PYGZus{}sig}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

Degeneracy analysis

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{Stats} \PYG{k+kn}{import} \PYG{n}{Degen\PYGZus{}plots}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Degen\PYGZus{}plots}\PYG{o}{.}\PYG{n}{merge\PYGZus{}components\PYGZus{}cd}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Degen\PYGZus{}plots}\PYG{o}{.}\PYG{n}{hist\PYGZus{}plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\section{Examples}
\label{\detokenize{Examples:examples}}\label{\detokenize{Examples::doc}}
In this short tutorial is shown the results of the ENIIGMA code applied to the YSO SVS 4\sphinxhyphen{}9, located in the SVS4 cluster at the Serpens molecular cloud.

The L\sphinxhyphen{}band spectrum data was taken from the VLT\sphinxhyphen{}ISAAC public database via \sphinxhref{http://www.stsci.edu/~pontoppi/ISAAC\_ARCHIVE\_PUBLIC.tar.gz}{this link}. The near\sphinxhyphen{}IR
photometric data is available in the \sphinxhref{https://irsa.ipac.caltech.edu/Missions/2mass.html}{2MASS catalog}.

\begin{sphinxadmonition}{note}{Note:}
These examples aim to show the code’s functionalities and should not be taken as an accurate scientific result!
\end{sphinxadmonition}


\subsection{Polynomial continuum}
\label{\detokenize{Examples:polynomial-continuum}}
In order to calculate the continuum SED an calculate the optical depth using the polynomial function, we can use the following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{Continuum} \PYG{k+kn}{import} \PYG{n}{Fit}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{filename} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/Users/will\PYGZus{}rocha\PYGZus{}starplan/eniigma\PYGZus{}doc/doc/Tutorial/svs49.txt}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Fit}\PYG{o}{.}\PYG{n}{Continuum\PYGZus{}poly}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+m+mf}{1.24}\PYG{p}{,} \PYG{l+m+mf}{4.0}\PYG{p}{,} \PYG{n}{order} \PYG{o}{=} \PYG{l+m+mf}{2.}\PYG{p}{,} \PYG{n}{range\PYGZus{}limits}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{1.24}\PYG{p}{,} \PYG{l+m+mf}{2.85}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{3.8}\PYG{p}{,} \PYG{l+m+mf}{4.}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

The parameters indicate that the continuum is calculated in the range between 1.24 to 4.0 microns, using a second\sphinxhyphen{}order polynomial function constrained by the ranges
between 1.24\sphinxhyphen{}2.85 and 3.8\sphinxhyphen{}4. microns.

The results are shown below:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=500\sphinxpxdimen]{{Continuum_svs49_poly}.pdf}
\caption{Continuum SED, SVS4\sphinxhyphen{}9 spectrum on the top panel, the optical depth on the bottom panel.}\label{\detokenize{Examples:id1}}\end{figure}

As you can note, the function fails to trace the continuum in the near\sphinxhyphen{}IR regime, althout it gives an acceptable continuum in the L\sphinxhyphen{}band interval. Below the same
procedure is repeated using the blackbody function.


\subsection{Blackbody continuum}
\label{\detokenize{Examples:blackbody-continuum}}
The Python commands to call the BB continuum function is:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{Continuum} \PYG{k+kn}{import} \PYG{n}{Fit}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Fit}\PYG{o}{.}\PYG{n}{Continuum\PYGZus{}BB}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+m+mf}{1.24}\PYG{p}{,} \PYG{l+m+mf}{4.}\PYG{p}{,} \PYG{n}{range\PYGZus{}limits}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{1.25}\PYG{p}{,} \PYG{l+m+mf}{2.5}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{3.8}\PYG{p}{,} \PYG{l+m+mf}{4.}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{guess} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mf}{1e\PYGZhy{}20}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

As used in the polynomial continuum case, the intervals are the same. The difference is that one BB function is used. The initial guesses to fit the spectrum are
the temperature of 1000 K and a scale factor of 1e\sphinxhyphen{}20. The results are shown below:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=500\sphinxpxdimen]{{Continuum_svs49_BB}.pdf}
\caption{Continuum SED, SVS4\sphinxhyphen{}9 spectrum on the top panel, the optical depth on the bottom panel.}\label{\detokenize{Examples:id2}}\end{figure}


\subsection{GA decomposition}
\label{\detokenize{Examples:ga-decomposition}}
The spectral decomposition using the genetic algorithm module reads the output from the polynomial continuum and BB function as well as an external file not calculated by the ENIIGMA code.
As example, let us use the output spectrum from the ENIIGMA code.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{GA} \PYG{k+kn}{import} \PYG{n}{optimize}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{filename} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Optical\PYGZus{}depth\PYGZus{}svs49.od}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{list\PYGZus{}sp} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H2O\PYGZus{}40K}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H2O\PYGZus{}NH3\PYGZus{}CO2\PYGZus{}CH4\PYGZus{}10\PYGZus{}1\PYGZus{}1\PYGZus{}1\PYGZus{}72K\PYGZus{}b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{d\PYGZus{}NH3\PYGZus{}CH3OH\PYGZus{}50\PYGZus{}10K\PYGZus{}I10m\PYGZus{}Baselined}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CO\PYGZus{}NH3\PYGZus{}10K}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{H2O\PYGZus{}CH4\PYGZus{}10\PYGZus{}0.6\PYGZus{}a\PYGZus{}V3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CO\PYGZus{}CH3OH\PYGZus{}10K}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{HNCO\PYGZus{}NH3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{optimize}\PYG{o}{.}\PYG{n}{ENIIGMA}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+m+mf}{2.84}\PYG{p}{,} \PYG{l+m+mf}{4.}\PYG{p}{,} \PYG{n}{list\PYGZus{}sp}\PYG{p}{,} \PYG{n}{group\PYGZus{}comb}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{skip}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{pathlib} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{)}
\end{sphinxVerbatim}

The optical depth used in this example is the file ‘Optical\_depth\_svs49.od’, and the initial guess for the laboratory data are listed in the list\_sp variable.
Once the files are set, the decomposition range is fixed for the interval between 2.84 and 4. microns, with a combination group of 3 experimental data in the final step of the code.
The keyword ‘skip=False’ means that the entire procedure will be executed. ‘pathlib=None’ means that the ice library is read from the folder download with the code. It is stored in your
Python directory.

In my laptop, 156 combinations were tested in a execution time of 75 seconds. The result is shown below:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=400\sphinxpxdimen]{{Final_plot}.pdf}
\caption{SVS 4\sphinxhyphen{}9 optical depth in black and the fitting model in green.}\label{\detokenize{Examples:id3}}\end{figure}


\subsection{Checking out the GA fitness evolution}
\label{\detokenize{Examples:checking-out-the-ga-fitness-evolution}}
The evolution of the optimisation over the generations for the combinations can be checked in two different ways. First, the 5 best combinations can be checked via this command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{GA} \PYG{k+kn}{import} \PYG{n}{check\PYGZus{}ga}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{check\PYGZus{}ga}\PYG{o}{.}\PYG{n}{top\PYGZus{}five\PYGZus{}scaled}\PYG{p}{(}\PYG{n}{savepdf}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

This will show the 5 best scaled fitness function in the same graph as seen below:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=400\sphinxpxdimen]{{graph_eniigma_top_five_scaled}.pdf}
\caption{GA evolution check.}\label{\detokenize{Examples:id4}}\end{figure}

Other checking options are available via this command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{GA} \PYG{k+kn}{import} \PYG{n}{check\PYGZus{}ga}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{check\PYGZus{}ga}\PYG{o}{.}\PYG{n}{check}\PYG{p}{(}\PYG{n}{combination}\PYG{o}{=}\PYG{l+m+mi}{78}\PYG{p}{,} \PYG{n}{option}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{4}\PYG{p}{)}
\end{sphinxVerbatim}

The figure below shows the evolution of the combination that gave the best solution, namely \sphinxhyphen{} 78.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=400\sphinxpxdimen]{{graph_eniigma_evol}.pdf}
\caption{GA evolution check for the best combination.}\label{\detokenize{Examples:id5}}\end{figure}

The population evolution can also be check over the generations and fitness function as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{GA} \PYG{k+kn}{import} \PYG{n}{check\PYGZus{}ga}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{check\PYGZus{}ga}\PYG{o}{.}\PYG{n}{check}\PYG{p}{(}\PYG{n}{combination}\PYG{o}{=}\PYG{l+m+mi}{78}\PYG{p}{,} \PYG{n}{option}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{9}\PYG{p}{)}
\end{sphinxVerbatim}

The result is shown below:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=400\sphinxpxdimen]{{graph_eniigma}.pdf}
\caption{Population check for the best combination.}\label{\detokenize{Examples:id6}}\end{figure}


\subsection{Evaluating the recurrence of the ice components}
\label{\detokenize{Examples:evaluating-the-recurrence-of-the-ice-components}}
The recurrence of the ice laboratory inside the confidence intervals can be addressed via pie charts. For example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{Stats} \PYG{k+kn}{import} \PYG{n}{Pie\PYGZus{}chart\PYGZus{}plots}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Pie\PYGZus{}chart\PYGZus{}plots}\PYG{o}{.}\PYG{n}{pie}\PYG{p}{(}\PYG{n}{sig\PYGZus{}level}\PYG{o}{=}\PYG{l+m+mf}{9.}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen,height=400\sphinxpxdimen]{{Pie_chart}.pdf}
\caption{Pie charts of the recurrence plots.}\label{\detokenize{Examples:id7}}\end{figure}

The values are given in percentage and means how many time a specific data was repeated in order to contribute to the selected confidence interval.


\subsection{Calculating confidence intervals and ice column densities}
\label{\detokenize{Examples:calculating-confidence-intervals-and-ice-column-densities}}
The confidence intervals can be visualised along with the spectral decomposition plot as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{ENIIGMA}\PYG{n+nn}{.}\PYG{n+nn}{Stats} \PYG{k+kn}{import} \PYG{n}{Stats\PYGZus{}Module}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{Stats\PYGZus{}Module}\PYG{o}{.}\PYG{n}{stat}\PYG{p}{(}\PYG{n}{f\PYGZus{}sig}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=400\sphinxpxdimen]{{Stats-1}.pdf}
\caption{Triangle plot showing the confidence intervals.}\label{\detokenize{Examples:id8}}\end{figure}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen,height=400\sphinxpxdimen]{{Stats-2}.pdf}
\caption{Optical depth and the minimum and maximum intervals.}\label{\detokenize{Examples:id9}}\end{figure}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=550\sphinxpxdimen,height=400\sphinxpxdimen]{{Stats-3}.pdf}
\caption{Optical depth decomposition indicating the used experimental data.}\label{\detokenize{Examples:id10}}\end{figure}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}